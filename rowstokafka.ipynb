{"paragraphs":[{"text":"%pyspark\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\nfrom pyspark.sql.functions import to_json, struct, col\nfrom random import randint, choice\nimport datetime\n\n # TODO: Ensure this is correct\nscala_version = '2.12' \nspark_version = '3.4.1'\ncsv_path = 's3a://jsifontes-cdp-data-east1/axo-spark-test/zeppelin/notebook/data.csv'\nkafka_broker = 'axo-kafka-test-corebroker2.jsifonte.a465-9q4k.cloudera.site:9093, axo-kafka-test-corebroker1.jsifonte.a465-9q4k.cloudera.site:9093, axo-kafka-test-corebroker0.jsifonte.a465-9q4k.cloudera.site:9093'\nkafka_user = 'jsifontes'\nkafka_pass = '1QAZ2wsx.'\nkafka_topic = 'axo-test-spark'\ntruststore_path = '/var/lib/cloudera-scm-agent/agent-cert/cm-auto-global_truststore.jks'\ntruststore_pass = 'XGYfrg376KH87dtPHb27Em700d'\n\n\n\npackages = [\n    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n    'org.apache.kafka:kafka-clients:3.2.0'\n]\nspark = SparkSession.builder\\\n   .master(\"local\")\\\n   .appName(\"certification-axo-spark-kafka\")\\\n   .config(\"spark.jars.packages\", \",\".join(packages))\\\n   .getOrCreate()\n\n# Read all lines into a single value dataframe  with column 'value'\n# TODO: Replace with real file. \n#df = spark.read.text(csv_path)\n\n\n\n\n\n\n","user":"jsifontes","dateUpdated":"2024-05-25T18:13:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1716656888099_906969941","id":"20240525-170808_1818147630","dateCreated":"2024-05-25T17:08:08+0000","dateStarted":"2024-05-25T18:13:58+0000","dateFinished":"2024-05-25T18:13:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:255","results":{"code":"SUCCESS","msg":[]}},{"text":"%pyspark\n\n# Esquema para el DataFrame de transacciones\nschema = StructType([\n    StructField(\"id_transaccion\", StringType(), nullable=False),\n    StructField(\"producto\", StringType(), nullable=False),\n    StructField(\"cantidad\", FloatType(), nullable=False),\n    StructField(\"monto\", FloatType(), nullable=False),\n    StructField(\"fecha\", TimestampType(), nullable=False)\n])\n\n# Funci√≥n para generar registros aleatorios de transacciones\ndef generar_transaccion():\n    productos = [\"producto1\", \"producto2\", \"producto3\", \"producto4\", \"producto5\"]\n    id_transaccion = randint(1, 1000)\n    producto = choice(productos)\n    cantidad = float(randint(1, 10))\n    monto = round(randint(10, 1000) * cantidad, 2)\n    fecha = datetime.datetime.now()\n    return id_transaccion, producto, cantidad, monto, fecha\n\n# Generar datos aleatorios y crear un RDD de transacciones\nrdd_transacciones = spark.sparkContext.parallelize([generar_transaccion() for _ in range(100)])\n\n# Crear DataFrame a partir del RDD y aplicar el esquema\ndf_transacciones = spark.createDataFrame(rdd_transacciones, schema)\n\ndf_transacciones = df_transacciones.select(col(\"id_transaccion\").alias(\"key\"), to_json(struct([col(c) for c in df_transacciones.columns])).alias(\"value\"))\n\n# Mostrar el DataFrame con las transacciones generadas\ndf_transacciones.show(truncate=False)","user":"jsifontes","dateUpdated":"2024-05-25T18:14:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1716656933869_-367249246","id":"20240525-170853_1596906550","dateCreated":"2024-05-25T17:08:53+0000","dateStarted":"2024-05-25T18:14:26+0000","dateFinished":"2024-05-25T18:14:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"%pyspark\n\n\n\n# Write to kafka\ndf_transacciones.write.format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", kafka_broker)\\\n  .option(\"topic\", kafka_topic)\\\n  .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n  .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n  .option(\"kafka.ssl.truststore.location\", truststore_path) \\\n  .option(\"kafka.ssl.truststore.password\", truststore_pass) \\\n  .option(\"kafka.sasl.jaas.config\", f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_user}\" password=\"{kafka_pass}\";') \\\n  .save()\n  \n  \n","user":"jsifontes","dateUpdated":"2024-05-25T18:14:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1716656953403_-1756136015","id":"20240525-170913_1744474140","dateCreated":"2024-05-25T17:09:13+0000","dateStarted":"2024-05-25T18:14:39+0000","dateFinished":"2024-05-25T18:14:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257","results":{"code":"SUCCESS","msg":[]}},{"text":"%pyspark\n","user":"jsifontes","dateUpdated":"2024-05-25T17:22:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1716657726246_166200072","id":"20240525-172206_667026010","dateCreated":"2024-05-25T17:22:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:258"}],"name":"csvtokafka","id":"2JXVUCMHH","noteParams":{},"noteForms":{},"angularObjects":{"livy3:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
